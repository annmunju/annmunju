# ML모델 : 모두연 강의

- XGBoost와 사이킷런을 활용한 그레디언트 부스팅

- 트리 모델 변천사
    - 의사결정나무
    - 배깅
    - 랜덤포레스트
    - 부스팅 : 시퀀셜을 추가
    - 그래디언트 부스팅 : 경사하강법을 추가
    - XGBoost, LightGBM
- 요즘은 부스팅 알고리즘이 대세
- 정형 데이터의 경우 머신러닝이 좋은 성능을 냄

## [의사결정나무](https://scikit-learn.org/stable/modules/tree.html#)
- 어떤 항목에 대한 관측값과 목표값을 연결시켜주는 모델로 결정 트리를 사용
- 분류트리, 회귀트리 모두 가능
- 결과를 해석하고 이해하기 쉽고 자료 가공할 필요가 거의 없다.
- 시각화 하기가 용이하다. 안정적이고 대규모의 데이터셋에서도 잘 동작한다.
- 주요 결정나무 알고리즘 : ID3, C4.5, C5.0, __CART__, CHAID, MARS 
- 의사결정 나무 성능평가
    - 불순도 : 분류된 모델이 얼마나 일관되게 결과를 맞추는지
    - Classifier 
        - 지니 계수 : 0일때 불순도가 없다. 0.5일때 불순도가 가장 높음. 루트 노드에 지니 불순도가 적혀있음. 그것이 0이 되면 아무것도 섞이지 않은 상태
        - 엔트로피 : 0일때 불순도가 없다. 최대 엔트로피는 클래스 개수에 따라 달라짐. (log2(N))
    - Regression
        - MSE
        - 푸아송 분포 : 예측하고자 하는 값(y)이 빈도수 일때 주로 사용
- Feature importance
    - tree 기반 알고리즘에서 사용. 
    - 데이터 누수 확인
    - 중요도 높은 피처들 확인
    - 불필요한 피처들
    - 모델의 해석성
    - 데이터 전처리
- 질문 : 루트 노드에 있는 특징이 가장 feature importance가 높은 특징과 다를 수 있음. 전자는 가장 초기 지니계수가 낮은 것, 후자는 전체에 대한 피처 중요도.

## Ensemble method
- 전처리 없이도 잘 돌아감
- boosting (sequential) 
    - 이전 결과를 바탕으로 실패한 것을 이용해 재훈련
- bagging (parallel)
    - 표본 집단을 다양하게 하여 병렬적으로 훈련
- 앙상블 모델 : 모델을 혼합해서 사용
- 성능 평가
    - 편향-분산 트레이드 오프
    - bagging은 분산을 감소시키기 위해 사용
    - boosting은 편향을 감소시키기 위해 사용
    - 데이터에 맞는 알고리즘 사용

## Bagging
- 부트스트랩(표본조사, 중복을 허용)을 통해 조금씩 다른 훈련데이터에 기초 분류기를 결합하는 방법
- 여러 트리들의 평균들을 이용하므로 노이즈에 강함
- OOB score 
- 시각화 지원 거의 불가능함.

### Random Forest

### Extremely Randomized trees
- 랜덤포레스트보다 빠르게 작동
- 렌덤포레스트는 모든 가능한 분할 지점 평가
- 엑스트라 트리는 랜덤하게 선택된 분할 지점 평가

## Boosting
- 이전 트리의 오차를 보안하는 알고리즘
- 깊이를 낮게해야 성능이 더 잘나옴. (과적합) 틀렸던 문제에 더 가중치를 주고 공부시키는 거니까
- XGBoost / LightBGM / CatBoost
- Adaboost
    - 분할을 한번만 한다.
    - 앞에서 틀린걸 바탕으로 웨이트를 업데이트.
- Gradient Boosting
    - 이전 학습기의 기울기를 활용. 손실함수의 기울기에 따라 가중치 업데이트

### Gradient Boosting
- loss update 시에 gradient를 활용
- train 데이터 셋으로 계산할 때 "잔차(residual)"라고 표현하고, 모델을 만들어 test 데이터를 평가할 때 "오차(error)"라고 표현
- gradient descent 오차를 보완하면서 최소화 해 나가는 방식으로 기울기를 변화시키는 방법
- 로스가 가장 적은것이 0이 되는 평가 방식을 주로 사용.
- squared loss vs absolute loss
    - 기울기가 있음 vs 기울기가 없음 : 최소가 되는 지점은 둘 다 있으나 전자는 기울기와 방향 둘 다 제공, 후자는 방향만 제공

### Scikit-learn에서 제공하는 Bossting 방법들
- XGBoost
    - 처리가 빠르다.
    - Early stopping 지원
    - 다양한 Hyper parameter 제공
    - GBM보다 느리다는 단점 존재
    - Overfitting 주의
    - 병렬처리 가능하도록 하드웨어 지원.
    - 파이썬 래퍼, 사이킷런 래퍼 각각 해당 모델 제공 (사용방법 다름/주로 파이썬 래퍼 사용)
- LightGBM
    - Histogram-based algorithm (결측치 있어도 오류 안남). 수치형 데이터를 범주화하니까
    - Gradient-based algorithm : 틀린거 위주로 샘플링(원사이클 샘플링)
    - Greedy Bundling, Feature Bundling
    - 균형을 맞추지 않고 트리가 한쪽으로 몰린채 생성될 수 있음. (틀린거 위주로 하기 때문에~) 
    - 속도 빠르고 메모리도 적게 사용. 병렬 처리 가능
    - 작은 데이터에 대해 과적합 되기 쉬움
        - over fitting parameter tuning
- CatBoost
    - categorycal feature을 다루는데 용이. light gbm도 가능 (전처리도 안해도됨)
    - Orderd boosting
    - 순열 기반 대안을 사용 (순차적 범주형 인코딩 용이)
    - 빠른 GPU gnsfus
    - 수치형 타입이 많을 때 시간이 많이 소요됨
    - 타겟 누수 문제를 해결하는데 도움.
- Histogram-based Gradient Boosting
    - 범주화하기 때문에 결측치가 있어도 작동함
    - sklearn에서 LightGBM 영향을 받아 만듦
    - 카테고리컬한 특징도 동작은 하지만 one-hot encoding을 하고 훈련하는 것을 권장

- 질문) 병렬로 트리를 만드는 배깅보다 순차적인 부스팅이 빠른 이유?
    - 성능 수렴 속도 : 부스팅은 모델 성능을 매번 개선하려고 함. 배깅은 각 모델이 독립적으로 동작해 성능 개선 느릴 수 있다.
    - 트리의 깊이 : 부스팅 알고리즘은 일반적으로 깊이가 얕은 트리
    - 과대적합 방지를 위한 early stop 
    - 최적화된 구현
    - 데이터 특정 부분에 집중해 모델을 만듦
    -> 일반적으로 보기는 어렵다. 상황에 따라서 배깅이 빠를 수 있다.

