## 추천시스템

### 협업 필터링 (collaborative filtering) (cf. content-based filtering : 컨텐츠 기반 필터링)
* 데이터
    * 유저 : 열 (j)
    * 아이템 : 행 (i)
    * 평가 여부 : r(행, 열) = 1(평가함)
    * 평점 : y^(행, 열) : 행번째 영화를 보고 열번째 사람이 평가한 점수
        * 평가 하지 않은 경우는 해당 점수 없음
    * feature (추가정보) : 유저 열에 추가 (x1, x2 ...)
        * x^(i) : i번째 영화에 대한 피쳐정보
* linear regression을 이용한 평점 예측
    * 평가하지 않은 경우를 예측하기 위해
    * w(j) * x(i) + b(j)
![이미지](./images/cf.png)

### Feature(X) 값 예측

* 아이템 특징인 X를 예측하려면
    * w(j) * x(i) + b(j) 에서 w(j), b(j)를 알고있음
    * 유저를 변수로 두고 cost 계산

* cost function 
    * 유저의 특징을 알아내는 cost function -> 아이템의 특징을 알아내는 cost function
    * 결국 [유저, 아이템] 별로 돌면서 유저 특징, 아이템 특징을 알아내는 cost function을 알게 됨 : J(w, b, x)

### Cosine Similarety

* 벡터 : list of numbers. (방향과 크기가 있는)
    * 벡터를 비교하고 싶을 때 출발점을 동일하게 해야함. 축들이 동일한 범위를 가지고 있어야 비교가 가능.

* 벡터의 내적 (inner product, dot product)
    * A, B 가 있을 때 A, B 직교한 길이와 한 변의 길이를 곱한 것과 동일
    * np.dot([1, 3], [5, 2]) = 1*5 + 3*2 = 11

* 코사인 유사도 = 벡터간의 각도 (COS(theta))
    * cos(theta) = np.dot / |A|*|B| 


### binary labels

* example
    * 구매 / 비구매
    * 좋아요 / X
    * 클릭 / X
    * ~초 이상 봤는가 / 안봤는가
* w(j) * x(i) + b(j) = y(i, j) 
    * y는 1 or 0
    * logistic regression과 동일하게 g(wx+b) 
        * g = sigmoid
* Logistic regression 의 Cost function
    * L(y, y_hat) = -y*(log y_hat) -(1-y)*(log 1-y_hat)

### mean normalization

* 이력이 없는 고객에게 추천을 해주고 싶을때는?
    * 아이템에 대해 내린 모든 유저들의 값 평균을 적용
1. w,b,x를 훈련하기 전에 각 아이템의 mean을 뺌
2. 훈련한 후 w(j) * x(i) + b(j) + [각 아이템의 평균 벡터]

### Custom Training Loop
* tf.GradientTape() as tape... 안에
    * auto diff
    * auto grad 가 있음
* optimizer
![cost](./cost%EC%A0%95%EC%9D%98%EB%A5%BC%20%EC%9E%98%ED%95%B4%EC%95%BC%ED%95%B4.png)