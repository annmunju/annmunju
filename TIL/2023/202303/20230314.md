## PCA 
- feature의 차원을 줄이고 싶을 때
- 구별하는데 도움이 되는 특징을 두드러지게 만들어줌
    - varies a lot (그 특징의 변이가 크다)
- 특징들 간에 비례하는 관계가 있는 경우에 그 특징들을 한번에 설명하는 새로운 축을 만들 수 있음
    - 그 새로운 축에 대한 해석을 할 수 있음
    - 새로운 축 = 기존 축들의 영향을 받은 축 : 이를 통해 해석을 해야함
- 모든 PCA 축은 직교하기 때문에 dot product 시에 0이 나옴 = Orthogonal matrix -> 모든 단위벡터 PCA 축은 Orthonomal matrix


## PCA 과정 
1. data centering
    - (X, Y)축 평균을 각각 빼줌
2. Scaling
    - 일반적으로 Z-scoring (X-m/시그마)
    - 원래 값을 찾기 용이함
3. 축(bases?)에 모든 값을 직교하게 하고 축을 돌려보면서 직교값들이 가장 퍼진 축을 채택
    - Principal component : (가장 퍼진 축) 주성분
4. 원래 좌표 값 * 축 벡터(단위 벡터, 유닛 벡터:벡터 길이가 1인 것) 내적을 구하면 새로운 축의 좌표 값을 구할 수 있음
5. 새로운 축에서 직교하는 축을 그음. 하나의 축은 하나의 설명만.
    - 각각의 축이 독립적인 설명을 할 수 있도록 완전 독립된 축을 만드는 것임.
    - 직교하는 두 벡터는 내적시 값이 0. (90도)


## dot product
- 내적 값은 크면 좋은것 (유사한 것)
    - 벡터간 각도가 작다
    - 내적 값이 작으면 벡터간의 각도는 큰 것 (유사함과 거리가 먼)
- 단위 벡터 : X / |X| = 1 이 됨.


## 선형 회귀와의 차이
- 선형 회귀 : input x -> output y
    - y 정답 - y 예측을 줄이는 방면으로 로스를 정의하고 로스를 감소시키는 축을 채택
- PCA : x1, x2 ... 축
    - 새로운 축에 직교하는 값들이 가장 퍼진 축을 채택


## PCA 축의 설명력
- 새로운 축의 값(원점과의 거리)을 기존 축(x1, x2) 좌표로 설명하기 위해서는 외적을 하면 됨
- 다만 외적을 할 때, 기존 점과 다르게 나올 수 있음.