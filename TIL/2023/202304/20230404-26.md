## 강화학습
* Agent : 학습의 주체
* Environment : Agent가 행동을 취할 수 있는 환경
* Action : Agent가 취할 수 있는 행동
* State : Agent가 현재 어떤 상태에 있는지를 나타내는 값
* Reward : Agent가 행동을 취했을 때 얻는 보상
* Policy function : Agent가 현재 상태에서 어떤 행동을 취할지 결정하는 함수
    * Policy : 어떤 상태일 때, 어떤 행동을 하는가.
* Value function : Agent가 현재 상태에서 얻을 수 있는 보상의 기댓값을 나타내는 함수
* Model : Agent가 학습 과정에서 추측하는 환경
* 강화학습은 결국 Agent가 환경과의 상호작용을 통해서 목표를 달성하는 방법을 배우는 문제

## 강화학습의 특징
1. 특정 행동에 대한 좋고 나쁨을 평가하는 보상이 주어짐
2. 현재의 의사결정이 미래에 영향을 미친다
3. 문제의 구조를 사전에 알 수 없다.

## 강화학습의 방법
* X : state s, Y : action a -> 지도학습의 관점으로 보면 최적은 아님.
* 엄격한 조건을 주는 것이 아닌, 잘했는가 못했는가에 대한 보상을 주기
    * 여기에서 보상은 정의하기 나름 (예. 떨어지면 -1000, 안떨어지면 +1)
* 주로 로봇, 주식 예측, 게임 등에 사용됨. (게임 비디오를 input으로 강화학습하기도 한다고 함)
* Return
    * Agent가 어떤 행동을 취했을 때 얻는 보상의 총합
    * Return = R0 + R1 + R2 + R3 + ... = R0 + (가중치)*R1 + (가중치)^2*R2 + (가중치)^3*R3 + ... 
        * 가중치(Discount factor)는 0~1 사이의 값
        * 일반적으로 0.9~0.99 사이의 값을 사용
    * 액션을 어떻게 취하느냐에 따라 return 값이 달라질 수 있음
    * Q(s, a) : State-action value function
        * s 상황에서 a 액션을 취했을 때의 return (결과가 최대일 때 해당 액션을 취함)
        * reward는 순간의 보상, return은 장기적으로 봤을 때 보상.
        * return은 미래의 보상을 포함하고 있기 때문에, 미래의 보상을 예측하는 것이 중요.
        * Q(s, a) = R0 + (가중치)*R1 + (가중치)^2*R2 + (가중치)^3*R3 + ...
* Policy Function
    * 지금 상태에서 어떤 액션을 취해야하는가를 결정하는 함수 
    * 계속 학습이 될 것.
    * Policy function은 state s를 input으로 받아서 action a를 output으로 내보냄. a = π(s)
    * 강화학습의 최종 목표는 결과들의 합이 최대가 되는 policy function을 찾는 것.



### (개념) Markov Decision Process
* 지금 상태가 다음 상태를 결정. 과거와 현재의 시간 관계를 모델링하는 것.
* 예시) 지금 날씨가 맑으면 내일 날씨가 맑을 확률이 높다.


### (개념) Expected value
* 기대값은 확률변수의 평균을 의미한다.
* E[X] = ∫xf(x)dx = ∑xP(x)


### (개념) Bellman Equation
* Q(s, a) = R(s, a) + γ * max_a'[Q(s', a')]
    * γ : discount factor
    * s' : 다음 상태
    * a' : 다음 액션
* 순서가 있는 경우에 가치를 계산하는 방법. 멀어지는 경우 가치가 작아지는 것을 반영.


### Discrete vs Continuous State
* 연속적인 상태에서는 위치와 이동뱡향(상태)가 나타나 있음.
* 움직임을 수학적으로 나타낼 때, Translation과 Rotation이 있음
    * X, Y, Z축으로 이동하는 Translation
    * X, Y, Z축으로 회전하는 Rotation. (roll, pitch, yaw)
