## 강화학습
* Agent : 학습의 주체
* Environment : Agent가 행동을 취할 수 있는 환경
* Action : Agent가 취할 수 있는 행동
* State : Agent가 현재 어떤 상태에 있는지를 나타내는 값
* Reward : Agent가 행동을 취했을 때 얻는 보상
* Policy function : Agent가 현재 상태에서 어떤 행동을 취할지 결정하는 함수
* Value function : Agent가 현재 상태에서 얻을 수 있는 보상의 기댓값을 나타내는 함수
* Model : Agent가 학습 과정에서 추측하는 환경
* 강화학습은 결국 Agent가 환경과의 상호작용을 통해서 목표를 달성하는 방법을 배우는 문제

## 강화학습의 특징
1. 특정 행동에 대한 좋고 나쁨을 평가하는 보상이 주어짐
2. 현재의 의사결정이 미래에 영향을 미친다
3. 문제의 구조를 사전에 알 수 없다.

## 강화학습의 방법
* X : state s, Y : action a -> 지도학습의 관점으로 보면 최적은 아님.
* 엄격한 조건을 주는 것이 아닌, 잘했는가 못했는가에 대한 보상을 주기
    * 여기에서 보상은 정의하기 나름 (예. 떨어지면 -1000, 안떨어지면 +1)
* 주로 로봇, 주식 예측, 게임 등에 사용됨. (게임 비디오를 input으로 강화학습하기도 한다고 함)
* Return
    * Agent가 어떤 행동을 취했을 때 얻는 보상의 총합
    * Return = R0 + R1 + R2 + R3 + ... = R0 + (가중치)*R1 + (가중치)^2*R2 + (가중치)^3*R3 + ... 
        * 가중치(Discount factor)는 0~1 사이의 값
        * 일반적으로 0.9~0.99 사이의 값을 사용
    * 액션을 어떻게 취하느냐에 따라 return 값이 달라질 수 있음
* Policy Function
    * 지금 상태에서 어떤 액션을 취해야하는가를 결정하는 함수 
    * 계속 학습이 될 것.
    * Policy function은 state s를 input으로 받아서 action a를 output으로 내보냄. a = π(s)
    * 강화학습의 최종 목표는 결과들의 합이 최대가 되는 policy function을 찾는 것.

### (개념) Markov Decision Process
* 지금 상태가 다음 상태를 결정.
* 예시) 지금 날씨가 맑으면 내일 날씨가 맑을 확률이 높다.