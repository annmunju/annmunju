# 20220923

- AI 강좌 필기 정리
> 용어
>> scaler : 1, 2, ...
>> vector : [1, 2, ...]
>> matrix : [[1, 2], [3, 4], ...]
>> tensor

---

## Feature scaling

- Feature 별로 bound가 다 다름
- w의 값이 비슷해야 빠르게 최적 값을 찾을 수 있음
    - 값의 차이가 많이 난다면 gradient descent에서 기울기를 계속 바꿔줘야함. 
    - 그래서 feature scaling을 해줌!
- feature과 weight의 관계
    - feature가 크면 weight는 작고
    - feature가 작으면 weight는 큰 특징이 있음


### 방법
> scailing을 각 피처별로 달리할 수도 있지만 보통은 함께 해줘서 기준이 동일해짐
1. max로만 나눠주기 : 가장 높은 값을 1로 만들어줌
2. mean normalization = min-max scaling
    - (x - u) / (max-min)
    - 중간 값을 빼서 x축, y축을 (0,0)에 가깝게 이동시키는 방법
    - 데이터의 패턴만 옮겨 주는 느낌 (이동)
3. z-score normalization
    - 표준편차를 나눠줘서 정규분포로 바꿔줌
    - 0~1 사이로 조정됨
> 정규분포 (가정)
>> 중심극한정리
>> 샘플의 개수가 늘어나면 샘플들의 평균은 정규분포를 따른다는 가정



### size
- min <= x <= max
- 최소값과 최대값이 큰 차이 없이 작고, 균열하다면 굳이 스케일링 할 필요는 없지 않을까


#
## gradient descent를 잘 작동하도록 하는 방법


### learning curve
- 이전에는 감소되는 변화량이 매우 작을 때 중지시켰음
- 잘못되는 경우
    - batch가 잘 안섞인 경우 똑같은 input만 들어가서 잘못 훌련되는 경우
    - learning rate가 지나치게 큰 경우
    - learning rate가 지나치게 작은 경우
    - weight에 변화량을 + 해버리는 경우 (직접 구현해야 하는 경우에 생길수도 있는 문제)
        - style transform
        - 내 사진과 화풍 사진을 섞는
- learning rate는 어떻게 선택?
    - 작은 값부터 넣어보기 (0.001 ...)


### Feature engineering
> 특성 공학

- 어디에 쓰나
    - feature 간의 상관관계가 있는 경우를 없애기 위해서 사용하기도 함
    - 유의미한 feature을 늘리기 위해서 사용하기도 함
- polynomial regression
    - f_w,b(x) = w1*x + w2*x^2 + b
    - 제곱하는 경우 모양이 정해져 있음
    - 목적에 맞게 다차식으로 변환해서 사용 가능
    - issue : rescaling 시에 범위가 제곱수일수록 커질 것
- feature correlation
    - 피처간 상호작용이 있는 경우 모델에 도움이 안됨
    - EDA시에 피처간 유사한 정도를 미리 파악해봐야 함
    - non linear regression의 경우 피처간 관계가 상관 없음 (뉴럴 네트워크)


### +. Long format vs Wide format
- long -> 위에 columns가 있고 아래에 example이 한줄씩 
    - 전체 데이터를 보기 위해서
- wide -> 위에 feature가 있고 