# 20221004

- AI 강좌 필기 정리

---

## review
- Linear Regression 과 Logistic Regression의 차이
    - Data의 차이
    - Model의 차이
- 이진분류에서 Linear Regression을 사용한 경우
    - 오판의 여지가 있음
    - 관측 불가능한 값이 나타남
    - 확률로 표현하고 싶을 때 0~1 사이의 값을 가져야하기 때문에
- Cost function
    - 왜 정의하나? 최적의 파라미터를 찾기 위해
    - 최적의 파라미터는? 비용을 최소화할 수 있도록 하는 값
    - 비용함수를 통해서 최적의 파라미터 w,b를 찾기 위해서 하는 방법 -> gradient descent (경사 하강법)
- slop (기울기) vs gradient (벡터 단위로 output/input (미분한 값의) 비율)

---

## Logistic regression Gradient descent

- 로지스틱 회귀분석의 J(cost function)은 y=0인 경우, y=1인 경우로 두가지 케이스의 값이 나옴
![gradient descent](/Users/anmunju/Documents/0_munju/mungdo/TIL/202209/imgs/gradient-descent.png)
- 방식은 선형 회귀와 동일

## problem of overfitting

### fit
- underfit : high bias (모델이 너무 반듯, 단순함)
    - bias : 예측값과 실제 정답과의 차이 평균
- fit : generalization이 잘 됨
- overfit : high variance (예측값의 변이가 큼)
    - variance : 예측값이 다양한 데이터 셋에 대해 얼만큼 변화할 수 있는지에 대한 양 (x값은 조금 변할때 예측값(y-hat)은 차이가 큰 경우 high)
- bias와 varience는 trade-off 관계

### addressing overfitting
> 과적합 된 모델의 경우 어떻게 적합하게 해야할까
1. 데이터 늘리기
2. feature selection
    - 모든 feature을 쓰며 data 수가 적은 경우 overfit 가능성이 높음
    - 유의미한 feature만 선택해서 사용 
        - 단점 : 중요한 피쳐를 놓칠 수도 있다.
3. Regularization (정규화)
    - 파라미터 weight의 사이즈를 줄이기
    - feature이 여러개인 경우 w를 감소시키면 varience가 줄어듦
- underfitting의 경우 더 좋은 모델을 사용

### Regularization 요약
- 목적 함수 J = (y - y-hat)^2 + (lambda*(w^2)) = 목적1 + 목적2
- gradient descent 함수에 J를 넣으면 알아서 w값을 줄이는 방향으로 미분하게 됨.
- Cost function에 가중치에 대한 제곱을 더해서 해당 가중치값을 감소시킴 (가중치 영향력이 작아짐)
- lambda : regularization parameter
    - lambda가 0이면 더해지는 가중치 값의 영향이 없기 때문에 w를 줄이지 않는다는 의미가 됨
    - lambda가 엄청 커진다면 더해지는 가중치 값의 영향이 매우 커져서 w를 많이 줄인다는 의미가 됨

[참고 L1규제와 L2규제](https://steadiness-193.tistory.com/262)