---
marp: true
title: 견고한 데이터 엔지니어링 (7장 요약)
description: 
author: annmunju
theme: default
paginate: true
---

# 견고한 데이터 엔지니어링
## 7장 요약

> 데이터 수집

---

# 7.1 데이터 수집이란?
> 데이터 수집과 데이터 통합의 차이
> 데이터 파이프라인 정의

- 데이터 파이프라인은 데이터 엔지니어링 수명 주기의 단계를 통해 데이터를 이동시키는 아키텍처, 시스템 및 프로세스의 조합이다.

---

# 7.2 수집 단계의 주요 엔지니어링 고려 사항

## 7.2.1 유한 데이터 vs 무한 데이터
- 비즈니스 프로세스는 데이터를 개별 배치로 인위적으로 잘라내 제한을 두어왔음.
- 모든 데이터는 유한해질 때 까지 무한함

## 7.2.2 빈도
- 배치 : 빈번함
- 마이크로 배치 : 꽤 빈번함
- 실시간 : 매우 빈번함 (스트리밍과 같은 의미로 사용)

---

## 7.2.3 동기 수집 vs 비동기 수집
- 동기 수집 : 원천 -> 수집 -> 대상(스토리지) 가 복잡한 의존성을 가지며 밀접하게 결합됨.
    - 중간 과정 중 하나라도 실패하면 다시 작업해야함.
- 비동기 수집 : 개별 이벤트 수준에서, 데이터가 들어오는 즉시, 처리할 수 있는 리소스(컴퓨터 자원)가 있다면 바로 처리

## 7.2.4 직렬화와 역직렬화
> 원천으로부터 데이터를 인코딩하고 송신 및 중간 저장 단계를 위해 데이터 구조를 준비

---

## 7.2.5 처리량과 확장성
- 처리량 확장에 서버, 샤드, 워커 등을 지원하는 관리형 서비스를 사용

## 7.2.6 신뢰성과 내구성
- 데이터 손실에 따른 영향과 비용으로 리스크를 미리 평가
- 직접 비용과 간접 비용 평가

---

## 7.2.7 페이로드
- 수집하려는 데이터셋의 종류, 형태, 크기, 스키마, 데이터 유형, 메타데이터 특성 알기
1. 종류(kind) : type, format
2. 형태(shape) : 표, json, txt, image, audio
3. 크기(size) : 바이트 수
4. 스키마와 데이터 유형
    - 애플리케이션의 클래스 구조로 감싸진 경우가 있고
    - API로 감싸져 있는 경우가 종종 있어, 애플리케이션 코드를 이해해야 하는 경우가 종종 있음
    - 업스트림 및 다운스트림 스키마 변경 검출과 처리
5. 메타데이터

---

### 업스트림 / 다운스트림 스키마 변경 검출과 처리

#### a. 개념
- **업스트림(Upstream)**: 데이터를 생성/제공하는 쪽 (예: 소스 DB, API).
- **다운스트림(Downstream)**: 데이터를 소비/활용하는 쪽 (예: DWH, ML 모델, 대시보드).

#### b. 스키마 변경 유형
- 열 추가 (Add column)
- 열 삭제 (Drop column)
- 열 타입 변경 (Change type)
- 열 이름 변경 (Rename column)
- 열 순서 변경

---

#### c. 변경 검출 방법
- 메타데이터 비교 (`INFORMATION_SCHEMA` 등)
- DWH 시스템 테이블 조회 (BigQuery, Snowflake 등)
- CDC(Change Data Capture) 로그 감지 (Debezium, Kafka Connect)
- 스키마 레지스트리 (Kafka, Glue, Confluent)

#### d. 변경 처리 전략
- **호환 가능 (추가)** → 무시하거나 `NULL` 처리, 자동 수용
- **호환 불가능 (삭제/타입 변경/이름 변경)**  
  - 스키마 에볼루션 지원 시스템 활용 (Hive, Delta Lake, Iceberg)  
  - 변환 레이어 도입 (새 스키마 ↔ 표준 스키마 매핑)  
  - 다운스트림 코드/쿼리 수정  
---

#### e. 실무 패턴
- 스키마 호환성 정책 정의 (예: 열 추가 허용, 삭제는 협의 필요)
- 자동 알림 (Airflow, DBT, Kafka Connect → Slack/Email)
- 스키마 버전 관리 (버전 태깅, 선택적 소비)
- 표준화 계층(Transform Layer) 운영

---

## 7.2.8 푸시 vs 풀 vs 폴링 패턴

- 푸시 전략은 원천에서 대상으로 보내줌
- 풀 전략은 대상에서 원천 데이터를 끌어옴
- 폴링은 변경 사항이 감지되면 풀과 같이 대상에서 원천을 끌어옴

---

# 7.3 배치 수집 고려 사항
- 시간 간격 배치 수집 : 일반적인 방법 (airflow 등). 야간에 데이터 한번에 처리 등
- 크기 기반 배치 수집 : IoT 데이터와 같은 실시간 데이터에서 주로 사용. 바이트 크기가 일정 부분 넘으면 수신 등

## 7.3.1 스냅숏 또는 차등 추출
- 전체 스냅숏시 갱신할 때마다 원천의 전체 현재 상태를 파악.
- 차등 갱신 == 증분 갱신. 마지막으로 읽은 이후의 갱신 및 변경 내용만 가져옴옴

---

## 7.3.2 파일 기반 익스포트 및 수집
- 파일 기반 익스포트는 푸시 방법. (원천 시스템에서 데이터를 밀어넣는 방식.)
- 원천 시스템에서 전처리 진행

## 7.3.3 ETL과 ELT
- Extract : 원천 시스템에서 데이터를 가져오는 것을 의미. 푸시, 풀 모두 해당됨
  - 추출시 메타데이터 읽기와 스키마 변경이 필요할 수 있음
- Load : 스토리지 대상에 적재하기. 변환 완료된 시점일 수 있고 이후 변환을 위해 적재하는 걸 수 있음.

---

## 7.3.4 입력, 갱신 및 배치 크기
- 소수 사용자가 대규모 행 변경 수행 < 다수 사용자가 소규모 행 변경 수행에 더 많은 성능 필요
  - ex. 트랜잭션 데이터베이스
- 따라서 데이터베이스, 스토어의 갱신 패턴을 알고 적절하게 사용 필요.

## 7.3.5 데이터 마이그레이션
- 기존 시스템과 새로운 시스템이 어떤 방식으로 각각 스키마를 처리하는지 알기

---

# 7.4 메시지 및 스트림 수집에 관한 고려 사항

## 7.4.1 스키마의 진화
- 스키마의 진화 해결 제안사항
  - 이벤트 처리 프레임워크에 스키마 레지스트리가 있는 경우 이를 사용해 스키마 변경 사항을 버전화.
  - 데드레터 큐를 사용해 처리되지 않은 이벤트와 관련된 문제 조사
  - 스키마 변경에 관해 업스트림 이해관계자와 지속적으로 소통하고 변경에 능동적 대처

---

## 7.4.2 늦게 도착하는 데이터
- 인터넷 전송 지연 등의 문제 흔히 발생
- 늦게 도착하는 데이터를 처리하려면 늦게 도착하는 데이터가 더이상 처리되지 않는 컷 오프 시간을 설정해야 한다.

## 7.4.3 주문 및 복수 전달
- 메시지는 최소 한 번 이상 전달. 중복 될 수 있음을 고려

## 7.4.4 재생
- 리플레이 기능. 특정 시간 범위에 대해 데이터를 재입력 혹은 재처리 할 수 있음
- 래빗MQ는 모든 사용자가 메시지를 소비한 후 메시지를 삭제

---

## 7.4.5 유효 시간
- 최대 메시지 보존 시간
- 너무 많은 백로그는 지연시간을 늘리고 성능을 저하시킴

## 7.4.6 메시지 크기
- 최대 메시지 크기를 처리할 수 있는지 확인해야함

## 7.4.7 에러 처리와 데드레터 큐
- 이벤트 처리 실패시 데드레터를 별도 보관하여 재처리

---

## 7.4.8 소비자 풀 앤 푸시
- Pull Subscription : 풀 방식의 구독. 사용자가 토픽에서 메시지를 읽음. 소비자(Consumer)가 메시지 큐에 직접 요청(polling) 해서 새 메시지가 있는지 확인
- 푸시 구독 : 서비스가 수신자에게 메시지를 사용할 수 있도록 함. 메시지 큐 시스템이 새로운 메시지가 도착하면 자동으로 소비자(Consumer)에게 전달(HTTP POST, gRPC 등)

## 7.4.9 위치
- 데이터를 수집하는 위치와 저장하는 위치, 분석하는 위치가 다를 수 있음
- 이때 전송하는 비용을 고려할 것
- 가까울수록 대역폭, 지연시간은 빠름

---

# 7.5 데이터 수집 방법

## 7.5.1 직접 데이터베이스 연결
- ODBC 또는 JDBC로 연결해 쿼리하고 읽어 수집용 데이터를 풀링해옴
- ODBC : DB 접근할 클라이언트가 호스팅하는 드라이버를 사용해 표준 ODBC API에 발행된 명령을 데이터베이스에 발행된 명령어로 변환. 
  - OS 및 아키텍처 네이티브 바이너리로 제공됨. (아키텍처/OS 별 버전 따로 관리하고 유지해야함)
- JDBC : 자바 드라이버. 개념적으로 ODBC와 유사. 
  - OS 및 아키텍처 간에 이식 가능한 표준 역할을 하는 JVM을 이용.
  - 단일 JDBC 드라이버로도 다양한 언어의 인터페이스로 활용 가능. (파이썬 코드로도 JVM에서 실행중인 JDBC 드라이버와 통신 가능)
- 현재 많은 DB가 ODBC/JDBC를 우회해 파케이, 아브로 등 형식으로 직접 데이터를 내보내는 네이티브 파일 익스포트 지원.

---

## 7.5.2 변경 데이터 캡쳐

- 배치 지향 CDC 
  - 마지막 변경된 시점 이후로 조회해 변경 내용을 가져올 수 있음.
  - 특정 시점 이후 변경된 행을 쉽게 판별할 수 있지만 해당 행에 적용된 모든 변경 사항을 반드시 얻을 수 있는 것은 아님.
  - 모든 기록이 남는 입력 전용 스키마를 통해 모든 트랜잭션을 확인

- 연속 CDC
  - 데이터베이스에 대한 각 쓰기를 이벤트로 처리.
  - PostgreSQL과 같은 트랜잭션 디비에 로그 기반 CDC를 통해 순차적으로 기록된 모든 변경 사항을 읽고 
  - 이벤트를 아파치 카프카 데베지움 스트리밍 플랫폼 등의 타깃으로 전송

---

- CDC와 데이터베이스 복제
  - 많은 DB들이 동기 복제를 지원. 해당 동기 복제를 이용해서 읽기 복제본을 생성
  - 읽기 복제본으로 운영 데이터베이스의 부하 없이 대규모 검색 실행
  - 비동기 CDC 복제는 분석 애플리케이션에서 사용

- CDC 고려사항
  - 유료임을 감안하기. 
  - 배치 처리는 부담이 될 수 있음

---

## 7.5.3 API
- 트렌드
  1. 많은 벤더사가 다양한 프로그래밍 언어를 위한 API 클라이언트 라이브러리 제공
  2. 다양한 데이터 커넥터 플랫폼이 SaaS, 오픈소스 등으로 제공됨
  3. 데이터 공유의 등장. 표준 플랫폼을 통해 데이터를 교환할 수 있음(물리적 이동 없이)
- API 직접 접근해야할 때, 커넥터 구축에 기회비용 발생할수도
  - 관리형 서비스의 사용자 지정 API 커넥터 구축을 사용

---

## 7.5.4 메시지 큐와 이벤트 스트리밍 플랫폼
- 배치 수집과 스트림 수집의 차이를 이해하기
  - 배치는 정적 워크플로 / 메시지와 스트림은 유동적
  - 유동적이라는 것은 데이터 게시, 소비, 재게시, 재소비와 함께 비선형적일 수 있음 
  -> 멱등성 보장이 필요함
- 실시간 데이터 파이프라인 처리량 고려하기
  - 가능한 짧은 지연시간으로.
  - 적절한 파티션 대역폭과 처리량 프로비저닝
  -> 관리형 서비스 고려하기

---

## 7.5.5 관리형 데이터 커넥터
- 서드 파티에 아웃소싱하자. (만들고 관리해야하니까)
- 일반적으로
  - 타깃 설정, 원천 설정
  - 수집
  - 권한 및 자격 증명, 갱신 주기 구성
  - 데이터 동기화 시작
  - 모니터링 제공

---

## 7.5.6 객체 스토리지로 데이터 이동
> 퍼블릭 클라우드의 객체 스토리지로 안전하게 파일 교환 처리하기

## 7.5.7 EDI
- 전자 문서 교환
- 이메일, 플래시 드라이브 같은 파일 교환 수단을 의미
- 자동화 처리를 통해서 내부 시스템으로 변환

## 7.5.8 데이터베이스와 파일 익스포트
> 익스포트하는 경우 대용량 데이터 스캔이 수반됨. 한 파티션씩 쿼리해 작은 단위로 익스포트 하거나 읽기 복제본을 사용해 부하를 줄이기.

---

## 7.5.9 공통 파일 형식의 실질적 문제
- 익스포트할 파일 형식. 
- CSV 안전하지 않음. 파일 인코딩과 스키마 정보를 설정
- 파케이, 아브로, 애로, ORC, Json 등

## 7.5.10 셸
> 명령어로 데이터 수집하기. 클라우드 벤더의 CLI 기반 도구 활용

## 7.5.11 SSH
> ssh 파일 전송에 사용하거나, ssh 터널로 DB 격리 연결

---

## 7.5.12 SFTP 및 SCP

## 7.5.13 웹훅
- 역 API 역할.
- 데이터 공급자가 이벤트가 생기면 데이터 소비자 쪽의 API 엔드포인트를 호출하는 방식 (공급자가 데이터를 Push)
- 공급자가 요청을 기다리지 않음. 소비자는 공급자의 웹훅 이벤트를 받아서 적재, 처리에 책임이 있음

---

## 7.5.14 웹 인터페이스

## 7.5.15 웹 스크레이핑
- 웹 스크레핑이 꼭 필요한지, 서드파티에서 데이터를 사용할 수 있는지 자문하기. DoS 공격이 될수도
- HTML 요소가 변경되면 갱신 어려움

## 7.5.16  데이터 마이그레이션용 전송 어플라이언스
- 스토리지 장비 이용. 100TB 정도면 이용하기

## 7.5.17 데이터 공유
> 데이터의 접근 권한만 제어해서 공유하는 것

---

# 7.6 함께 일할 담당자

## 7.6.1 업스트림 이해관계자

## 7.6.2 다운스트림 이해관계자

---

# 7.7 드러나지 않는 요소

## 7.7.1 보안

## 7.7.2 데이터 관리
- 스키마 변경 : git 버전 관리
- 데이터 윤리, 개인정보보호, 컴플라이언스

---

## 7.7.3 데이터옵스
- 모니터링
  - 관심 있는 시간 간격당 생성된 이벤트 수
  - 각 이벤트의 평균 크기 등
  - 클라우드 운영 중단시 대응책
- 데이터 품질 테스트
  - 잘못된 데이터의 장애 : 데이터 재앙
  - 필드의 의미가 변경되는 경우 등 왜곡된 데이터 관리

---

## 7.7.4 오케스트레이션

## 7.7.5 소프트웨어 엔지니어링

