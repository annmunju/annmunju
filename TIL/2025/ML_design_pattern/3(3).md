<!-- 136p~ -->

## 3.3 디자인 패턴 7: 앙상블

### 문제

- 머신러닝에는 세가지 오류가 생길 수 있다.  
    - 줄일 수 없는 오류 : 데이터셋 노이즈, 잘못된 문제정의, 측정 오류, 혼재 요인 등의 Input 문제
    - 줄일 수 있는 오류
        - 편향으로 인한 오류 : 편향은 모델이 특징과 라벨 관계에 대해 충분히 학습할 수 없게 만듦. (높은경우) 과소적합 유발
        - 분산으로 인한 오류 : 보이지 않는 새로운 예에 대한 일반화 어렵게 만듦. (높은경우) 과대적합 유발
        -> 모델은 낮은 편향과 낮은 분산 추구. 두가지 달성이 어려워 "편향-분산 트레이드오프"라고 함
- 아주 큰 모델에 대해서는 편향-분산 트레이드오프가 잘 발생하지 않음
- 중소 규모의 문제에서 편향-분산 트레이드오프를 어떻게 줄일 수 있을까?

### 솔루션

- 앙상블: 여러 머신러닝 모델을 결합하고 그 결과를 집계하여 예측하는 머신러닝의 기술

#### 배깅

- 병렬 앙상블 방법. 높은 분산 해결
- 여러개의 하위 모델과 여러개의 개별 샘플로 훈련시켜 이에 모델 출력 결과를 집계.
- 회귀의 경우 출력 결과의 평균을, 분류 작업의 경우 과반수 클래스를 집계

#### 부스팅

- 많은 용량을 가진 앙상블 모델 구성. 편향 감소 효과.
- 강한 학습기를 만들기 위한 약한 학습기 반복 개선
- 동일한 모델을 여러번 잔차 예측하고 이에 따른 개선된 모델 학습 -> 반복 작업.
- 대표적인 부스팅 알고리즘으로 AdaBoost, Gradient Boosting Machine, XGBoost이 있음

#### 스태킹

- 여러 모델의 출력 결합.
- 초기 모델은 전체 학습 데이터셋을 학습 -> 2차 메타 모델 학습 

```python
members = [model_1, model_2, model_3]

n_members = len(members)
for i in range(n_members):
    model  = fit_model(members[i]) # 미리 정의한 train_data 일부 훈련
    model.save(f"models/model_{i+1}.h5")

for i in range(n_members):
    model = members[i]
    for layer in model.layers:
        layer.trainable = False
        layer._name = f"ensemble_{i+1}_{layer.name}"

member_inputs = [model.input for model in members]
member_outputs = [model.output for model in members]

# output 취합
merge = layers.concatenate(member_outputs)

# Meta-Model을 훈련
hidden = layers.Dense(10, activation='relu')(merge)
ensemble_output = layers.Dense(1, activation='relu')(hidden)
ensemble_model = Model(inputs=member_inputs, outputs=ensemble_output)
ensemble_model.compile(loss='mse', optimizer='adam', metrics=['mse'])
```

### 작동 원리

#### 배깅
- 개별 모델 (상관관계가 낮은 모델)이 서로 다른 오류를 가지기 때문에 이 결과들을 평균하면 오류들이 상쇄되고 예측이 정답에 더 가까워진다
- 앙상블의 오류는 모델 개수에 따라 선형적으로 감소함.
- 신경망에는 효과적이고, KNN/SVM에는 덜 효과적임 
    - 안정적인 모델에서는 학습 데이터가 변경되어도 모델 결과가 크게 바뀌지 않음
    - 불안정적인 모델에서는 분산이 높아 학습 데이터가 변경되면 결과가 크게 바뀜 -> 모델들이 다양해 오류들의 상관관계가 낮음 -> 앙상블 효과가 큼

#### 부스팅
- 모델을 반복 개선해 예측 오류를 줄임.
- 점점 복잡한 모델이 복잡해지고 결과로 생성되는 앙상블 모델은 가장 큰 모델이 됨.
- 편향은 과소 적합 경향과 관련되어 있음
- 처음에는 단순한(편향된) 모델로 시작하되, 그 모델이 틀린 문제(예측하기 어려운 예제)를 다음 모델이 집중적으로 다시 풀게 시켜서, 최종적으로는 복잡한 문제도 잘 맞히는 똑똑한 모델을 만든다

#### 스태킹
- 배깅과 유사하게 단순 모델 평균화를 확장한 것. 
- 배깅은 앙상블의 개별 모델이 모두 동일한데 반면, 스태킹의 단순 모델 평균화는 서로 다른 유형일 수 있다.
    - 스태킹은 가중 평균을 취하도록 평균화 단계를 수정해 필요한 모델에 더 큰 가중치를 부여
- 스태킹은 여러 ML 모델의 출력을 2차 모델의 입력으로 전달하는 학습 기술

### 트레이드 오프와 대안

- 늘어난 학습 시간과 설계 시간
    - 어떤 아키텍처를 얼마나 사용할지? 
    - 프로덕션에 투입시에 유지 관리, 추론 복잡성, 리소스 사용량이 늘어남. 개발 비용도 늘어남.
    - 선형 모델 혹은 DNN 모델과 비교하고 적절히 채택할 것

- 드롭아웃을 통한 배깅
    - 드롭아웃이란 신경망에서 학습의 각 미니배치에 대해 네트워크 뉴런을 무작위로 끄는 방법. (서로 조금씩 다른 작은 모델 생성)
    - 배깅의 본질은 서로 다른 모델 만들어서 평균내는거
    - 드롭아웃 적용해 훈련시킨 모델은 배깅과 유사한 방식이라는 것. 

- 모델 해석 가능성의 감소
    - 이건 해결 가능한건 아님. 단순 모델로 바꾸는 수 밖에...

- 문제에 맞는 도구 선택하기
    - 편향 처리에 부스팅, 분산 수정에 배깅.
    - 상관관계가 높은 오류가 있는 두 모델을 결합하는 것은 분산을 낮추는데 도움이 되지 않는다.
