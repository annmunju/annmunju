## Dataproc에서 Spark 실행

### 1. 하둡 생태계
- 컴퓨터 클러스터를 만들고 분산 처리를 사용하면서 시작됨
- HDFS : 클러스터의 머신에 데이터를 분산 저장하는 파일시스템. 맵리듀스로 데이터 분산 처리
- Hadoop : 여러 컴퓨터 클러스터에 있는 대규모 데이터 세트의 분산처리를 위한 프레임워크
- Spark : 일괄 및 스트리밍 데이터 처리를 위한 고성능 분석 엔진. 인메모리 처리
- 온프렘은 물리적 클러스터 확장에 제약이 있어 한계있음 -> Dataproc은 관리형 하둡, 스파크 환경

### 2. Dataproc에서 하둡 실행하기
- 낮은 비용 : 시간당 클러스터당 가상 CPU당 1센트. 초단위 과금. 최소 결제 기간 1분
- 초고속 : 작업 시간 평균 90초 이내
- 클러스터 크기 조정 
- 오픈소스 생태계
- 통합 : 스토리지 서비스와 통합되어 데이터 손실 없음
- 관리형 : REST API를 통해 클러스터 및 spark 작업 등에 상호작용
- 버전 관리 
- 고가용성
- 개발자 도구 제공
- 초기화 작업 : 필요 설정과 라이브러리 설치 등의 맞춤 설정 가능
- 자동 또는 수동 구성 : 선택 구성 가능. Anaconda, Hive WebHCat, Jupyter 노트북, Zeppelin 노트북 Druid, Presto, Zookeeper 등

- 클러스터 만드는 방법 예시
    - https://github.com/GoogleCloudPlatform/dataproc-templates

- 온프레미스 하둡과 유사한 구조를 가짐
    - 노드 = VM + HDFS 스토리지
- 클러스터 구성 요소
    - 관리자 노드 : 클러스터 전체 제어 / 오케스트레이션
    - 워커 노드 : 실제 데이터 저장 + 작업 실행
        - 주 작업자 노드 : 워커 노드 중 HDFS 저장소를 포함한 핵심 그룹
        - 보조 작업자 그룹 : 선택적 워커 노드 그룹. 작업 실행만 담당
            - 보조 작업자 그룹은 선점형 보조 작업자(spot 같은거) / 비선점형 보조 작업자(vm 같은거) 둘 중 하나로 선택해서 사용. 보통 선점형 씀.
    - 관리형 인스턴스 그룹 : 워커 노드를 동일한 탬플릿으로 묶어 관리
- 오토스케일링 기능
- 클러스터 종료하면 HDFS 스토리지는 사라짐. 그래서 외부 스토리지 연결해서 사용.
    - HDFS는 HDDs 그리고 SSDs로 이뤄지고 영구 디스크임
    - 빅테이블 연결 가능
    - 분석할거면 빅쿼리 연결 가능
- Dataproc 사용 이벤트 순서 : 설정 -> 구성 -> 최적화 -> 활용 -> 모니터링
    - 설정 : 클러스터 만들기. 기본 노드 (관리자 노드).  최대 워커 노드 수는 할당량 및 각 작업자에 연결된 SSD의 수에 따라 결정
    - 구성 : 기본 노드, 워커 노드, 선점형 워커 노드 각각 옵션을 따로 지정할 수 있음.
- 클러스터 맞춤화
    - 초기화 스크립트 작성. 메타데이터 지정.
    - 커스텀 머신 유형, 커스텀 이미지 선택.
    - 영구 SSD 부팅 디스크 사용
- 작업 제출 (Job)
    - ui, cli, api, Dataproc workflow tamplates, cloud composer(airflow 기반)
    - 직접 하둡 인터페이스로 제출하면 안됨. 인식 못함

### 3. HDFS 대신 클라우드 스토리지 사용하기
- 온프렘 하둡은 데이터를 최대한 CPU 가까이 두는 구조 -> 디스크 부족해 스토리지 늘리려면 계산 노드도 강제로 늘려야해서 비효율
- 클라우드 Dataproc으로 변경시 처음에는 HDFS 그대로 두고 Lift & Shift 가능. 하지만 장기적으로 HDFS는 단점 존재

- HDFS 대신 클라우드 스토리지를 사용하면
    - 컴퓨팅과 스토리지가 분리되어 사용하지 않으면 꺼버림. (탄력적)
    - 내구성과 통합성 (다른 GCP 서비스와 바로 연결)
    - 마이그레이션이 용이함
    - 하지만 작은 파일/블록 많이 다루는 워크로드에 부적합하고 느릴 수 있음. 그리고 객체 저장소 특성상 완전히 새 객체로 교체해야함. (디렉토리 이름 변경, 파일 rename 동작 비효율적)

- 데이터 이동 전략 (Disk CP)
    - 푸시 기반 vs 풀 기반

### 4. Dataproc 최적화하기
- 데이터와 클러스터 위치 일치
    - 성능의 핵심은 데이터가 저장된 리전 = 클러스터 리전.
    - Dataproc 자동 영역 선택은 편리하지만, Cloud Storage 버킷 위치와 반드시 맞춰야 네트워크 비용·지연 최소화.
- 네트워크 병목 방지
    - GCS ↔ Compute Engine 사이에는 대규모 네트워크 파이프 존재.
    - VPN 게이트웨이나 잘못된 네트워크 경로를 거쳐 트래픽이 제한되지 않도록 주의.
- 입력 파일과 파티션 최적화
    - 너무 작은 파일을 많이 두지 말 것.
    - 1만 개 이상 작은 파일은 큰 파일로 합치기 → 파티션 수 과도 증가 방지.
    - 대규모 데이터(파티션 5만 개 이상) 처리 시 fs.gs.block.size를 크게 조정해 성능 개선.
- 디스크 크기 고려
    - 영구 디스크(PD) 성능은 크기에 비례.
    - 작은 용량 디스크는 처리량이 제한적 → 워크로드 크기에 맞는 적정 용량 선택 필요.
- 클러스터 VM 크기·수 조정
    - “얼마나 많은 VM이 필요한가?”는 실제 워크로드로 프로토타입 실행 후 비교가 정답.
    - 클라우드의 장점은 필요할 때만 확장·축소 가능하다는 것.
    - 따라서 작업 단위(Job scope)에 맞춰 클러스터 크기를 조정하는 전략이 일반적.

### 5. Dataproc 스토리지 최적화하기
- HDFS vs Cloud Storage
    - HDFS
        - 메타데이터 작업 많은 경우
        - 추가 쓰기가 많은 경우
        - 지연에 민감한 경우 (I/O가 한자릿수 ms 수준으로 필요할 때)
    - Cloud Storage
        - 빅데이터 파이프라인의 입/출력 저장소 역할
    - 결론
        - 초기 입력, 마지막 출력은 Cloud Storage가. 중간 저장은 HDFS가
- HDFS 잘 쓰는 법
    - 로컬 HDFS를 최소한만 쓸 경우 → 기본 PD를 줄여 비용 절감.
    - 공간이 부족할 경우 → 기본 PD 늘리기 (단, 성능 효과 없음).
    - 성능이 중요한 워크로드 → SSD 추가해서 로컬 HDFS로 쓰기.
        -> 진짜 I/O 집약적인 워크로드면, 표준 PD 늘리기 대신 SSD 추가가 맞다.
- 클러스터 운영 전략
    - 영구 클러스터 방식: 온프레미스처럼 유지 → 관리 복잡, 비용 큼, GCP 서비스와 통합 제한적
    - 임시 클러스터(Recommended):
        - 데이터를 Cloud Storage에 두고,
        - 작업 단위로 클러스터 생성 → 작업 끝나면 삭제
        - 필요한 동안만 리소스를 쓰므로 비용 절감, 유연성 확보
    -> 영구 클러스터가 필요하다면 작게 만들고 최소화하는 게 낫다.

---

## 워크플로 탬플릿 사용
> YAML 파일에 워크플로 템플릿을 정의
https://cloud.google.com/dataproc/docs/concepts/workflows/use-workflows?hl=ko

```yaml
jobs:
- hadoopJob:
    args:
    - teragen
    - '1000'
    - hdfs:///gen/
    mainJarFileUri: file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar
  stepId: teragen
- hadoopJob:
    args:
    - terasort
    - hdfs:///gen/
    - hdfs:///sort/
    mainJarFileUri: file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar
  stepId: terasort
  prerequisiteStepIds:
    - teragen
placement:
  managedCluster:
    clusterName: my-managed-cluster
    config:
      gceClusterConfig:
        zoneUri: us-central1-a
```

```bash
gcloud dataproc workflow-templates instantiate-from-file \
    --file=TEMPLATE_YAML \
    --region=REGION
```

## 모니터링 최적화하기
- Spark 작업 실패 원인 찾기 ->  Cloud 콘솔 또는 gcloud 명령어를 사용하여 드라이버 프로그램 출력을 검색할 수 있음
- 빨리 찾으려면 각 Dataproc 작업에 고유한 라벨을 만들어 사용하면 됨.
- spark.sparkContext.setLogLevel 으로 로그 수준을 정의할 수 있음