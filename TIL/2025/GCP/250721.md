### The Extract, Load, and Transform Data Pipeline Pattern

ELT 아키텍처 다이어그램
- 데이터가 올라오면 여러번 변환 수행 가능 
- Dataform 프로그래밍 옵션 사용해서 간소화된 변환 가능
- 빅쿼리 스테이징 로드되면 → SQL 스크립트나 SQL 워크플로 사용하여 빅쿼리 내 변환 적용

빅쿼리 SQL 스크립팅 및 일정 예약기능
- 빅쿼리 UDF를 이용해 SQL이나 JavaScript를 사용한 커스텀 데이터 변환 가능
- 프로시저 사용 가능. Apache Spark용 저장 프로시저 실행 지원 (스토리지 파일에 python, java 등으로 작성한 코드 사용하거나 sql 인라인으로 실행 가능)
- 원격함수는 cloud run functions과 통합해 사용 가능. 더 복잡한 transform 구문 실행 가능. 원격함수 등록 후 sql에서 그대로 사용. + 노트북 사용도 가능

---

### The Extract, Transform, and Load Data Pipeline Pattern

ETL 아키텍처 다이어그램
- 빅쿼리 로드 전에 조정 및 변환 

ETL GUI (dataprep, data fusion)
- dataprep은 노코드 솔루션. 함수를 체인처럼 연결해 레시피 만듦. 예약과 모니터링 가능. 인터페이스로 변환 적용 전에 영향 확인 가능.
- data fusion은 온프렘과 클라우드에서 여러 데이터 소스와 연결. 드래그 앤 드롭으로 파이프라인 빌드. 플러그인 허용 및 하두브 스파크 클러스터에서 실행

dataproc (배치 데이터 처리)
- 하둡과 스파크 워크로드를 실행. hdfs 파일을 다룰 수 있음 → 스토리지, 빅쿼리, Nosql 저장.
- 여러 스토리지 옵션 제공. 탬플릿 사용하면 편리하게 작업 가능 (ymal) → gcp 제출하려면 gcloud 명령줄 도구 사용.

spark용 dataproc serverless
-  스파크 클러스터 관리, 자동 확장 종량제 가격, 빠른 배포 → 사용자는 코드 작성에만 신경쓰면 됨.
- 배치용 서버리스 모드와 대화형 노트북 서버리스모드 제공. 
- 메타스토어를 영구 스토리지로 활용. 데이터 스토리지 검색을 위해서 클라우드 스토리지 사용. 

스트리밍 데이터 처리 옵션
- message bus (pub/sub, 비동기식 커뮤니케이션 지원) → streaming job → 변환과 보강
- 아파치 빔. 일괄데이터, 스트리밍 모두 처리. 프로그래밍 언어 지원. 템플릿 작성 용이하게 제공.

bigtable이 데이터 파이프라인에서 하는 역할
- 빅테이블 → 스트리밍 데이터 처리 . 유연한 스키마 설계 . 처리량 높고 지연시간 짧은 시계열 데이터, IoT, 금융 거래 데이터 등에 용이.

---

### Automation Techniques

파이프라인 자동화 패턴과 옵션
- 예약된 워크로드 자동화. BigQuery Dataform
- Cloud Storage에 파일을 업로드하면 Dataproc을 사용한 일괄 처리 후 Cloud Storage에 도착
- 예약된 작업이나 일회성 작업에 대해서는 Cloud Scheduler와 Cloud Composer 활용

cloud scheduler와 워크플로
- 지정된 반복주기로 워크로드 호출. http/s 호출, pub/sub 메세지, workflows로 트리거. workflow는 깃 액션처럼 생겼네

cloud composer 기능과 사용 사례
- Airflow와 Cloud Composer를 사용하면 Python으로 간편하게 워크플로를 개발하고 실행할 수 있음
- Composer는 작업 실행을 추가로 관리하고 오류 처리, 재시도, 모니터링 같은 기능을 통합해 운영을 원활하게 

cloud run functions의 기능
- aws lambda와 거의 동일한 역할 수행

eventarc의 기능과 자동화 사용 사례
- aws eventbridge와 비슷한 역할. 느슨한 연결로 이벤트 중심 아키텍처 설계
