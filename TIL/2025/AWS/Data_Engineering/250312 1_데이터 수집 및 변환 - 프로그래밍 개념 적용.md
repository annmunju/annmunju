
## 1.4: 프로그래밍 개념 적용
데이터 파이프라인의 구현, 테스트 및 배포를 위해 지속적 통합 및 지속적 전달을 사용하는 방법
데이터 소스 쿼리 및 데이터 변환에 SQL 쿼리를 사용하는 방법과 반복 가능한 배포를 위해 AWS 코드형 인프라 서비스를 사용하는 방법을 파악
분산 컴퓨팅과 그래프 데이터 구조 및 트리 데이터 구조와 같은 다양한 데이터 구조 및 알고리즘을 사용하는 방법

### 데이터의 도구와 대상
- SQL를 사용할 것인지, Spark와 같은 코드 기반 접근할 것인지 선택
- Spark 사용
	- Spark Streaming: 데이터를 배치 또는 거의 실시간 처리
	- Spark SQL
	- Spark ML
- AWS에서 Spark를 실행하는 방법
	- Glue 사용
	- EMR을 통해 실행용 클러스터 배포
	- ECS, EKS를 통한 컨테이너 환경에서 엔진 실행

### 케이스1) 데이터 변환 대상이 Redshift에 있는 경우
- SQL 쓸건지 Spark 쓸건지
- 데이터 변환 작업은 데이터 웨어하우스인 Redshift 내부에서 수행됨
	- 데이터 처리 엔진이 변환을 수행하는 동안 Spark SQL을 사용하는 방식도 가능
	- 그러니까, 중간 변환 단계를 Spark로 넘기고 데이터 웨어하우스에 로드하는 방식
- 그럼 왜 이렇게 처리? 복잡한 쿼리 실행시에 redshift가 오래 걸릴 수 있으니까
	- redshift의 쿼리 최적화 방식
	1. 제출된 SQL 쿼리를 parser 및 optimizer를 통해 라우팅하여 쿼리 계획을 개발함
	2. 실행 엔진이 쿼리 계획을 코드로 변환하고 실행을 위해 해당 코드를 컴퓨팅 노드로 보냄
- EMR을 사용하는 경우
	- 사용자 지정 가능한 Hadoop, Spark, HBase, Presto와 같은 프레임워크를 사용할 수 있음
- Data Pipeline 서비스
	- 종속 프로세스 정의. EMR 작업, SQL 쿼리와 같은 활동 또는 비즈니스 로직이 포함됨


---

## 개요
- 데이터 파이프라인 및 프로그래밍 개념을 활용하여 데이터를 유용하게 만드는 방법 학습
- ETL 프로세스 (수집, 변환, 로드) 운영화 및 자동화

## 데이터 변환
- AWS는 다양한 데이터 변환 엔진 및 방법 제공
- 데이터 엔지니어는 원시 데이터세트 사용, 결합, 분석 엔진으로 처리, 새로운 데이터세트 생성
- 모든 것은 비즈니스 목적, 요구 사항, 정보에 따라 결정

## SQL vs 코드 기반 접근 방식 (Spark)
- **SQL**:
    - 장점: 표준 언어, 오랜 기간 사용
    - 단점: 복잡한 처리에는 한계
- **Spark**:
    - 장점: 강력하고 활용도 높음, 복잡한 데이터 처리 요구 사항에 적합
    - 단점: SQL보다 코딩 필요

## 환경 고려 사항
- SQL 의존 환경 -> SQL 사용
- 지연 시간, 처리량 요구 사항 높은 복잡한 데이터 처리 -> Spark 사용

## AWS에서 Spark 실행 방법
- Spark Streaming (배치/실시간 처리)
- Spark SQL (표준 SQL, Spark ML 사용)
- AWS Glue (서버리스 Spark 실행)
- Amazon EMR (Spark 클러스터 배포, 관리형 서비스)
- Amazon ECS, Amazon EKS (컨테이너 환경)
- DataBricks (AWS 파트너 관리형 서비스)

## 시나리오별 선택
- Amazon Redshift 데이터 변환 대상:
    - ELT (Amazon Redshift 내부 SQL 기반 쿼리 사용)
    - Spark SQL (변환을 Spark 클러스터로 오프로드)

## 쿼리 최적화
- 대량 데이터 복잡한 쿼리 -> 처리 시간 증가
- Amazon Redshift 쿼리 최적화:
    1. 파서, 옵티마이저를 통해 쿼리 계획 개발
    2. 실행 엔진이 쿼리 계획을 코드로 변환, 컴퓨팅 노드로 전송

## 대규모 데이터세트 처리 엔진
- Amazon EMR: Hadoop, Spark, HBase, Presto 등 프레임워크 사용, 단일 서버에서 수천 개 노드로 확장 가능
- Yarn, Tez, Pig 등 Hadoop 도구 실행 가능

## AWS Data Pipeline (관리형 ETL 서비스)
- AWS 서비스 및 온프레미스 리소스 간 데이터 이동 및 변환 정의
- 데이터 노드 (데이터 저장 위치), 순차적 실행 작업 (EMR 작업, SQL 쿼리 등) 정의
- 예시: S3 클릭스트림 데이터를 Redshift로 이동

## AWS Glue Studio
- SQL 기반 변환 ETL 작업 설계 지원 (시각적 인터페이스)
- Spark 코딩 기술 없이 Spark 엔진 사용 가능

## AWS Glue DataBrew
- 일련의 파일에 변환 적용

## Athena
- Amazon S3에 저장된 정형/반정형 데이터 쿼리
- 영숫자 문자 및 밑줄 포함 열 이름 지원 (특수 문자 미지원)
- 단순 쿼리만 지원*

## 코드형 인프라 (IaC)
- 오케스트레이션 시스템 핵심 개념, 데이터 엔지니어링 수명 주기 모든 단계 적용
- 코드 최적화 (데이터 수집/변환 런타임 축소)

## 코드 최적화 전략
- 병렬 처리 (AWS Glue Spark 사용)
- 배치 처리 (AWS Glue, Amazon EMR 사용)
- 열 기반 데이터 형식 (Parquet, ORC)
- 필터링 기술 (불필요 데이터 제거)
- 데이터 파티셔닝 (날짜, 리전 기준)
- 데이터 압축 (AWS Glue 내장 옵션)
- 리소스 할당 최적화 (메모리, 병렬 처리 설정)
- 쿼리 최적화 (인덱싱, 쿼리 재작성)
- 증분 처리 (변경 데이터만 처리)
- CloudWatch 사용 성능 모니터링

## 인프라 배포 자동화
- IaC: CloudFormation, AWS CDK 사용
- CI/CD: 데이터 파이프라인 구현, 테스트, 배포 자동화
- AWS SAM: 서버리스 데이터 파이프라인 패키징 및 배포
- 버전 관리: 템플릿 정의 파일 소스 코드 리포지토리에 커밋, CI/CD 파이프라인 자동화

## 서버리스 애플리케이션 최적화
- 병렬화 및 동시성 활용 성능 개선
- Lambda 함수 최적화 (지연 시간 단축, 처리량 증가)

## Lambda 함수 구성
- 코드 최적화, 최신 AWS SDK/라이브러리 사용
- 적절한 메모리/CPU 할당
- 동시성 제한 구성 (워크로드, 성능 요구 사항 기반)
- 비동기식 호출 (즉각적인 응답 불필요)
- 프로비저닝된 동시성 (함수 사전 워밍)
- 적절한 시간 제한 값 설정
- 재시도 동작 구성 (일시적 오류 처리)
- Amazon EBS 프로비저닝된 IOPS (높은 I/O 성능)
- VPC 내부 리소스 액세스 시 지연 시간 고려 (함수를 동일 VPC에 배치)

## Kinesis Data Streams 데이터 손실 복구
- 멱등성 처리 사용, 샤드 반복기가 오류 발생 전 샤드 위치 가리키도록 Java 애플리케이션 코드 수정
    
## 자동화된 테스트
- 데이터 변환 정확성, 무결성 보장
- 데이터 품질 검사, 스키마 검증

## 데이터 구조 및 알고리즘
- 효율적, 확장 가능한 데이터 처리 시스템 설계/구현에 필수
- 성능 최적화, 확장성 개선, 처리 시간 단축
- 다양한 데이터 구조 이해, 특정 사용 사례에 적합한 구조 선택
- 각 알고리즘은 데이터 액세스 방법, 명령 응답 방법 제공

## MapReduce 예시
- Amazon EMR, Hadoop 분산 파일 시스템 사용 데이터 저장
- MapReduce 알고리즘 사용 데이터 처리, 인사이트 추출
- 데이터 파티셔닝, 캐싱, 병렬 처리로 파이프라인 성능 최적화

## AWS에서 프로그래밍 개념 적용 예시
- 데이터 수집: Lambda 사용하여 데이터 수집 태스크 처리
- 데이터 처리: Amazon EMR, AWS Glue 사용하여 데이터 처리 작업 정의 (Python, Scala)
- 실시간 데이터 처리: Kinesis 사용하여 스트리밍 데이터 사용/처리
- 데이터 변환: AWS Glue 사용하여 복잡한 변환 수행 (데이터 정리, 집계, 정규화)
- 데이터 저장: Amazon RDS, Amazon Redshift (정형), Amazon S3 (반정형/비정형) 사용
- 워크플로 오케스트레이션: Step Functions 사용하여 서버리스 워크플로 구축